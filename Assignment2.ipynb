{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "from itertools import combinations\n",
    "import copy\n",
    "import argparse\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def load_instance(\n",
    "    source: str =\"/home/jdnunes/Documents/Heuristics&MetaHeuristics/SCP-Instances\",\n",
    "    instance: str ='scpd5.txt'\n",
    "):\n",
    "    with open(os.path.join(source, instance)) as f:\n",
    "        data = f.readlines()\n",
    "        \n",
    "    return data \n",
    "\n",
    "\n",
    "def redundancy_elimination(solution, sets, current_cost, costs):\n",
    "        # list of redundant subsets\n",
    "        redundant = []\n",
    "        # iterate over the set of subsets\n",
    "        for i, set1 in enumerate(list(solution.keys())):\n",
    "            # compare current position with all others\n",
    "            flags = [False for elem in solution[set1]]\n",
    "            for j, set2 in enumerate(list(solution.keys())[i+1:]):\n",
    "                for k, elem in enumerate(solution[set1]):\n",
    "                    if elem in sets[set2]:\n",
    "                        flags[k]=True\n",
    "            # if all elements of subset1 covered in other subsets, append subset1 to redundant\n",
    "            if all(flags)==True:\n",
    "                redundant.append(set1)\n",
    "        for set_ in redundant:\n",
    "            # remove the redundant subsets from the sets\n",
    "            solution.pop(set_)\n",
    "            # update the cost\n",
    "            current_cost-=costs[int(set_)-1]\n",
    "\n",
    "        return solution, current_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):  \n",
    "    # first line : nattr, nsets\n",
    "    for i, elem in enumerate(data):\n",
    "        data[i] = [int(e) for e in elem.strip().split(\" \")]\n",
    "\n",
    "    # second line: costs\n",
    "    costs = []\n",
    "    for i, elem in enumerate(data[1:]):\n",
    "        if len(elem)==1 and len(data[i+2])>1:\n",
    "            break\n",
    "        costs.extend(elem)\n",
    "    \n",
    "    del data[1:i+1]\n",
    "    data.insert(1, costs)\n",
    "\n",
    "    # following lines (MxK), with M=nattr, and K=num sets that cover attribute m: nsets, list(sets)\n",
    "\n",
    "    # 1: store de row indices that indicate nsets\n",
    "    indices=[]\n",
    "    for j, line in enumerate(data[2:-1]):\n",
    "        idx = j+2\n",
    "        if len(line)==1 and len(data[idx+1])>1:\n",
    "            # a line with 1 element is only an index if the next line contains more than 1 element\n",
    "            # otherwise, it could still be a set (e.g. 25 (12+12+1))\n",
    "            indices.append(idx)\n",
    "    # 2: iterate over the sets:\n",
    "    sets_temp = []\n",
    "    indices.append(len(data))\n",
    "    for i, idx in enumerate(indices[:-1]):\n",
    "        s = []\n",
    "        for j, elem in enumerate(data[idx+1:indices[i+1]]):\n",
    "            s.extend(elem)\n",
    "        sets_temp.append(s)\n",
    "    \n",
    "    sets = {}\n",
    "    \n",
    "    for i, set_list in enumerate(sets_temp):\n",
    "        attr = i+1\n",
    "        for set_ in set_list:\n",
    "            if str(set_) in list(sets.keys()):\n",
    "                sets[str(set_)].append(attr)\n",
    "            else:\n",
    "                sets[str(set_)] = [attr]\n",
    "    \n",
    "    del data[2:]\n",
    "    data.extend(sets)\n",
    "    m=data[0][0]\n",
    "    universe = list(range(1, m+1))\n",
    "    costs =  data[1]\n",
    "    return data, universe, sets, costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2: Improvement Heuristic for the Set Covering Problem (SCP)\n",
    "import sys\n",
    "\n",
    "def ih_best_neighbour(solution, universe, sets, cost, costs, k=10, topk=True):\n",
    "    \n",
    "    # do best neighbour local_search while cost is improving\n",
    "    improving = True\n",
    "    if topk:\n",
    "        topk_costs = [(cost-i) for i in range(k)]\n",
    "        topk_sols = [solution for _ in range(k)]\n",
    "    while improving:\n",
    "        improving = False\n",
    "        # list of set pairs to swap\n",
    "        keys = list(solution.keys())\n",
    "        aggregated_sets = [comb for comb in combinations(keys, 2)]\n",
    "\n",
    "        new_solution = copy.deepcopy(solution)\n",
    "        # initialize best_cost\n",
    "        best_cost = copy.deepcopy(cost)\n",
    "        \n",
    "        for j, pair_of_sets in enumerate(aggregated_sets):\n",
    "            # remove next k subsets fom solution\n",
    "            reduced_solution = copy.deepcopy(solution)\n",
    "            previous_cost = 0 # cost of the two sets that will be removed\n",
    "            for j in range(2):\n",
    "                reduced_solution.pop(pair_of_sets[j])\n",
    "                previous_cost+=costs[int(pair_of_sets[j])-1]\n",
    "            # assess attributes not covered in reduced_solution\n",
    "            uncovered = copy.deepcopy(universe)\n",
    "            for k,v in reduced_solution.items():\n",
    "                for elem in v:\n",
    "                    if elem in uncovered:\n",
    "                        uncovered.remove(elem)\n",
    "            # solve the reduced set covering\n",
    "            # select sets that cover the uncovered attributes\n",
    "            covered = []\n",
    "            sub_solution = dict()\n",
    "            sub_cost=0\n",
    "            search_sets = dict()\n",
    "            # reduce search space for the sets that cover the still uncovered attributes\n",
    "            for k, v in sets.items():\n",
    "                if any(item in v for item in uncovered):\n",
    "                    search_sets[k]=v\n",
    "            # assess every feasible solution for the reduced set size covering problem\n",
    "            \n",
    "            # limit the neighborhood to all feasible solutions that add 2 sets to the reduced size set covering problem\n",
    "            all_set_combs = [comb for comb in combinations(search_sets.keys(), 2)]\n",
    "            # all_set_combs.extend([comb for comb in combinations(search_sets.keys(), 3)])\n",
    "            feasible_set_combs = []\n",
    "            for comb in all_set_combs:\n",
    "                # check if is a feasible solution\n",
    "                attlist = [search_sets[k] for k in comb]\n",
    "                covered_attributes = [v for vlist in attlist for v in vlist]\n",
    "                if all(item in covered_attributes for item in uncovered):\n",
    "                    feasible_set_combs.append(comb) # append to feasible set combs\n",
    "                    \n",
    "            for comb in feasible_set_combs:\n",
    "                sub_cost = 0\n",
    "                for k in comb:\n",
    "                    sub_cost+=costs[int(k)-1]\n",
    "                curr_cost = cost - previous_cost + sub_cost\n",
    "                if curr_cost < best_cost:\n",
    "                    new_solution = copy.deepcopy(reduced_solution)\n",
    "                    s = {k:search_sets[k] for k in comb}\n",
    "                    new_solution.update(s)\n",
    "                    best_cost = curr_cost\n",
    "                    new_solution, best_cost = redundancy_elimination(new_solution, sets, best_cost, costs)\n",
    "                    del topk_sols[topk_costs.index(max(topk_costs))]\n",
    "                    topk_costs.remove(max(topk_costs))\n",
    "                    topk_costs.append(best_cost)\n",
    "                    topk_sols.append(new_solution)\n",
    "                    improving = True\n",
    "           \n",
    "        solution = copy.deepcopy(new_solution)\n",
    "        cost = copy.deepcopy(best_cost)\n",
    "    if topk:\n",
    "        return solution, list(solution.keys()), cost, topk_sols, topk_costs\n",
    "    return solution, list(solution.keys()), cost\n",
    "\n",
    "\n",
    "def ih_first_neighbour(solution, universe, sets, cost, costs, K=2):\n",
    "    \n",
    "    # do best neighbour local_search while cost is improving\n",
    "    improving = True\n",
    "    while improving:\n",
    "        improving = False\n",
    "        # list of set pairs to swap\n",
    "        keys = list(solution.keys())\n",
    "        aggregated_sets = [comb for comb in combinations(keys, 2)]\n",
    "\n",
    "        new_solution = copy.deepcopy(solution)\n",
    "        # initialize best_cost\n",
    "        best_cost = copy.deepcopy(cost)\n",
    "        \n",
    "        for j, pair_of_sets in enumerate(aggregated_sets):\n",
    "            if improving:\n",
    "                break\n",
    "            # remove next k subsets fom solution\n",
    "            reduced_solution = copy.deepcopy(solution)\n",
    "            previous_cost = 0 # cost of the two sets that will be removed\n",
    "            for j in range(2):\n",
    "                reduced_solution.pop(pair_of_sets[j])\n",
    "                previous_cost+=costs[int(pair_of_sets[j])-1]\n",
    "            # assess attributes not covered in reduced_solution\n",
    "            uncovered = copy.deepcopy(universe)\n",
    "            for k,v in reduced_solution.items():\n",
    "                for elem in v:\n",
    "                    if elem in uncovered:\n",
    "                        uncovered.remove(elem)\n",
    "            # solve the reduced set covering\n",
    "            # select sets that cover the uncovered attributes\n",
    "            covered = []\n",
    "            sub_solution = dict()\n",
    "            sub_cost=0\n",
    "            search_sets = dict()\n",
    "            # reduce search space for the sets that cover the still uncovered attributes\n",
    "            for k, v in sets.items():\n",
    "                if any(item in v for item in uncovered):\n",
    "                    search_sets[k]=v\n",
    "            # assess every feasible solution for the reduced set size covering problem\n",
    "            \n",
    "            # limit the neighborhood to all feasible solutions that add 2 sets to the reduced size set covering problem\n",
    "            all_set_combs = [comb for comb in combinations(search_sets.keys(), 2)]\n",
    "            # all_set_combs.extend([comb for comb in combinations(search_sets.keys(), 3)])\n",
    "            feasible_set_combs = []\n",
    "            for comb in all_set_combs:\n",
    "                # check if is a feasible solution\n",
    "                attlist = [search_sets[k] for k in comb]\n",
    "                covered_attributes = [v for vlist in attlist for v in vlist]\n",
    "                if all(item in covered_attributes for item in uncovered):\n",
    "                    feasible_set_combs.append(comb) # append to feasible set combs\n",
    "                    \n",
    "            for comb in feasible_set_combs:\n",
    "                sub_cost = 0\n",
    "                for k in comb:\n",
    "                    sub_cost+=costs[int(k)-1]\n",
    "                curr_cost = cost - previous_cost + sub_cost\n",
    "                if curr_cost < best_cost:\n",
    "                    new_solution = copy.deepcopy(reduced_solution)\n",
    "                    s = {k:search_sets[k] for k in comb}\n",
    "                    new_solution.update(s)\n",
    "                    best_cost = curr_cost\n",
    "                    new_solution, best_cost = redundancy_elimination(new_solution, sets, best_cost, costs)\n",
    "                    improving = True\n",
    "                    break\n",
    "           \n",
    "        solution = copy.deepcopy(new_solution)\n",
    "        cost = copy.deepcopy(best_cost)\n",
    "        \n",
    "    return solution, list(solution.keys()), cost\n",
    "\n",
    "# Assignment 3: Escaping Local Optima\n",
    "\n",
    "def geometric_cooling(t, alpha=0.99):\n",
    "    return alpha*t\n",
    "\n",
    "\n",
    "def simulated_annealing(\n",
    "    solution,\n",
    "    universe,\n",
    "    sets,\n",
    "    cost,\n",
    "    costs,\n",
    "    K=2,\n",
    "    t=50,\n",
    "    max_iter=10000,\n",
    "):\n",
    "    \n",
    "    # do best neighbour local_search while cost is improving\n",
    "    improving = True\n",
    "    # count iterations\n",
    "    \n",
    "    step = 0\n",
    "    for j in range(max_iter):\n",
    "        if not improving:\n",
    "            break\n",
    "        improving = False\n",
    "        # list of set pairs to swap\n",
    "        keys = list(solution.keys())\n",
    "        aggregated_sets = [comb for comb in combinations(keys, 2)]\n",
    "\n",
    "        new_solution = copy.deepcopy(solution)\n",
    "        # initialize best_cost\n",
    "        best_cost = copy.deepcopy(cost)\n",
    "        \n",
    "        for i, pair_of_sets in enumerate(aggregated_sets):\n",
    "            if improving:\n",
    "                break\n",
    "            # remove next k subsets fom solution\n",
    "            reduced_solution = copy.deepcopy(solution)\n",
    "            previous_cost = 0 # cost of the two sets that will be removed\n",
    "            for j in range(2):\n",
    "                reduced_solution.pop(pair_of_sets[j])\n",
    "                previous_cost+=costs[int(pair_of_sets[j])-1]\n",
    "            # assess attributes not covered in reduced_solution\n",
    "            uncovered = copy.deepcopy(universe)\n",
    "            for k,v in reduced_solution.items():\n",
    "                for elem in v:\n",
    "                    if elem in uncovered:\n",
    "                        uncovered.remove(elem)\n",
    "            # solve the reduced set covering\n",
    "            # select sets that cover the uncovered attributes\n",
    "            covered = []\n",
    "            sub_solution = dict()\n",
    "            sub_cost=0\n",
    "            search_sets = dict()\n",
    "            # reduce search space for the sets that cover the still uncovered attributes\n",
    "            for k, v in sets.items():\n",
    "                if any(item in v for item in uncovered):\n",
    "                    search_sets[k]=v\n",
    "            # assess every feasible solution for the reduced set size covering problem\n",
    "            \n",
    "            # limit the neighborhood to all feasible solutions that add 2 sets to the reduced size set covering problem\n",
    "            all_set_combs = [comb for comb in combinations(search_sets.keys(), 2)]\n",
    "            # all_set_combs.extend([comb for comb in combinations(search_sets.keys(), 3)])\n",
    "            feasible_set_combs = []\n",
    "            for comb in all_set_combs:\n",
    "                # check if is a feasible solution\n",
    "                attlist = [search_sets[k] for k in comb]\n",
    "                covered_attributes = [v for vlist in attlist for v in vlist]\n",
    "                if all(item in covered_attributes for item in uncovered):\n",
    "                    feasible_set_combs.append(comb) # append to feasible set combs\n",
    "                    \n",
    "            for comb in feasible_set_combs:\n",
    "                sub_cost = 0\n",
    "                for k in comb:\n",
    "                    sub_cost+=costs[int(k)-1]\n",
    "                curr_cost = cost - previous_cost + sub_cost\n",
    "                p = np.exp(-((curr_cost-best_cost)/t))\n",
    "                if (curr_cost < best_cost) or (np.random.random() < p):\n",
    "                    new_solution = copy.deepcopy(reduced_solution)\n",
    "                    s = {k:search_sets[k] for k in comb}\n",
    "                    new_solution.update(s)\n",
    "                    best_cost = curr_cost\n",
    "                    new_solution, best_cost = redundancy_elimination(new_solution, sets, best_cost, costs)\n",
    "                    improving = True \n",
    "                step+=1\n",
    "                # update the temperature parameter every 2 steps\n",
    "                if (step%2)==0:\n",
    "                    t = geometric_cooling(t)\n",
    "        solution = copy.deepcopy(new_solution)\n",
    "        cost = copy.deepcopy(best_cost)\n",
    "        \n",
    "    return solution, list(solution.keys()), cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_avg_pc_deviation(heuristic, parent_dir, out, instances):\n",
    "    \n",
    "    f1 = \"/home/jdnunes/Documents/Heuristics&MetaHeuristics//Assignment 2//results//scp_best.txt\"\n",
    "    f2 = os.path.join(parent_dir,\"results\",out+\"-\"+ heuristic+\".txt\")\n",
    "    df1 = pd.read_csv(f1, sep=\" \", header=None, names=[\"id\", \"cost\"])\n",
    "    df1 = df1.loc[df1['id'].isin(instances)]\n",
    "    df2 = pd.read_csv(f2, sep=\" \", header=None, names=[\"id\", \"cost\"])\n",
    "    c1 = np.array(list(df1[\"cost\"]), dtype=np.float32)\n",
    "    c2 = np.array(list(df2[\"cost\"]), dtype=np.float32)\n",
    "    pc_dev = ((c2 / c1) - 1)*100\n",
    "    avg_pc_deviation = np.mean(pc_dev)\n",
    "    for i, inst in enumerate(instances):\n",
    "        with open(os.path.join(parent_dir,\"results\",out+\"-\"+ heuristic+\"-pc-dev.txt\"), \"a\") as f:\n",
    "            f.write(str(inst)+' '+str(pc_dev[i])+'\\n')\n",
    "    print(f\"Average percent deviation from best known solutions ({heuristic}): {avg_pc_deviation:.2f} %\")\n",
    "\n",
    "def ttest(parent_dir, groups):\n",
    "    \n",
    "    f1 = os.path.join(parent_dir,\"results\",groups[0]+\".txt\")\n",
    "    f2 = os.path.join(parent_dir,\"results\",groups[1]+\".txt\") \n",
    "    \n",
    "    df1 = pd.read_csv(f1, sep=\" \", header=None, names=[\"id\", \"cost\"])\n",
    "    df2 = pd.read_csv(f2, sep=\" \", header=None, names=[\"id\", \"cost\"])\n",
    "    \n",
    "    a = df1[\"cost\"].to_numpy()\n",
    "    b = df2[\"cost\"].to_numpy()\n",
    "    _, pval = scipy.stats.ttest_ind(a, b)\n",
    "    \n",
    "    return pval\n",
    "\n",
    "\n",
    "def options():\n",
    "   \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--parent_dir\", type=str, default=\"/home/jdnunes/Documents/Heuristics&MetaHeuristics/Assignment 2\")\n",
    "    parser.add_argument(\"--out\", type=str, default=\"scp-ch4\")\n",
    "    parser.set_defaults()\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    instances = [\n",
    "    \"4.2\", \"4.3\", \"4.4\", \"4.5\", \"4.6\", \"4.7\", \"4.8\", \"4.9\",\n",
    "    \"5.1\", \"5.2\", \"5.3\", \"5.4\", \"5.5\", \"5.6\", \"5.7\", \"5.8\", \"5.9\",\n",
    "    \"6.1\", \"6.2\", \"6.3\", \"6.4\", \"6.5\",\n",
    "    \"A.1\", \"A.2\", \"A.3\", \"A.4\", \"A.5\",\n",
    "    \"B.1\", \"B.2\", \"B.3\", \"B.4\", \"B.5\",\n",
    "    \"C.1\", \"C.2\", \"C.3\", \"C.4\", \"C.5\",\n",
    "    \"D.1\", \"D.2\", \"D.3\", \"D.4\", \"D.5\",\n",
    "    ]\n",
    "    args = options()\n",
    "    \n",
    "    start = time()\n",
    "\n",
    "    bi_elapsed_time = 0\n",
    "    ch_elapsed_time = 0\n",
    "    fi_elapsed_time = 0\n",
    "    sa_elapsed_time = 0\n",
    "    for inst in instances:\n",
    "        \n",
    "        instance = 'scp'+''.join(inst.split('.')).lower() +'.txt'\n",
    "        \n",
    "        data = load_instance(instance=instance)\n",
    "        \n",
    "        _, universe, sets, costs = process_data(data)\n",
    "        \n",
    "        # initial constructive heuristic (incumbent solution)\n",
    "        ch_start = time()\n",
    "        init_solution, _, cost = ch4(universe, sets, costs, True)\n",
    "        ch_end = time()\n",
    "        ch_elapsed_time += (ch_end - ch_start)\n",
    "        \n",
    "        ## improvement heuristic (best neighbour)\n",
    "        bi_start = time()\n",
    "        bi_solution, _, bi_cost, topk_sols, topk_costs = ih_best_neighbour(init_solution, universe, sets, cost, costs)\n",
    "        bi_end = time()\n",
    "        bi_elapsed_time += (bi_end - bi_start)\n",
    "        \n",
    "        \n",
    "        # simulated_annealing\n",
    "        sa_start = time()\n",
    "        best_sa_cost = bi_cost\n",
    "        # GRASP: restart simulated annealing k=10 times and keep the solution with the best cost\n",
    "        for i, sol in enumerate(topk_sols):\n",
    "            sa_solution, _, sa_cost = simulated_annealing(sol, universe, sets, topk_costs[i], costs)\n",
    "            if sa_cost < best_sa_cost:\n",
    "                best_sa_cost = sa_cost\n",
    "                best_sa_solution = sa_solution\n",
    "        \n",
    "        sa_end = time()\n",
    "        sa_elapsed_time += (sa_end - sa_start)\n",
    "        # improvement heuristic (first neighbour)\n",
    "        #fi_start = time()\n",
    "        #fi_solution,_,fi_cost = ih_first_neighbour(init_solution, universe, sets, cost, costs)\n",
    "        #fi_end = time()\n",
    "        #fi_elapsed_time += (fi_end - fi_start)\n",
    "        #with open(os.path.join(args.parent_dir,\"results\",args.out+\".txt\"), \"a\") as f:\n",
    "        #       f.write(str(inst)+' '+str(cost)+'\\n')\n",
    "        #with open(os.path.join(args.parent_dir,\"results\",args.out+\"-fi.txt\"), \"a\") as f:\n",
    "        #       f.write(str(inst)+' '+str(fi_cost)+'\\n')\n",
    "        with open(os.path.join(args.parent_dir,\"results\",args.out+\"-simulated-annealing.txt\"), \"a\") as f:\n",
    "               f.write(str(inst)+' '+str(best_sa_cost)+'\\n')\n",
    "    end = time()\n",
    "    print(f\"Finished! Total elapsed time: {end - start:.3f} s\")\n",
    "    print(f\"Constructive heuristic elapsed time (total): {ch_elapsed_time:.3f} s\")\n",
    "    #print(f\"First improvement elapsed time (total): {fi_elapsed_time:.3f} s\")\n",
    "    print(f\"Simulated Annealing elapsed time (total): {sa_elapsed_time:.3f} s\")\n",
    "    \n",
    "    print(f\"Constructive heuristic elapsed time (average): {ch_elapsed_time/len(instances):.3f} s\")\n",
    "    # print(f\"First improvement elapsed time (average): {fi_elapsed_time/len(instances):.3f} s\")\n",
    "    print(f\"Simulated Annealing elapsed time (average): {sa_elapsed_time/len(instances):.3f} s\")\n",
    "    \n",
    "    # print_avg_pc_deviation(\"fi\", args.parent_dir, args.out, instances)\n",
    "    print_avg_pc_deviation(\"simulated-annealing\", args.parent_dir, args.out, instances)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def print_avg_pc_deviation(heuristic, parent_dir, out, instances):\n",
    "    \n",
    "    f1 = \"/home/jdnunes/Documents/Heuristics&MetaHeuristics//Assignment 2//results//scp_best.txt\"\n",
    "    f2 = os.path.join(parent_dir,\"results\",out+\"-\"+ heuristic+\".txt\")\n",
    "    df1 = pd.read_csv(f1, sep=\" \", header=None, names=[\"id\", \"cost\"])\n",
    "    df1 = df1.loc[df1['id'].isin(instances)]\n",
    "    df2 = pd.read_csv(f2, sep=\" \", header=None, names=[\"id\", \"cost\"])\n",
    "    c1 = np.array(list(df1[\"cost\"]), dtype=np.float32)\n",
    "    c2 = np.array(list(df2[\"cost\"]), dtype=np.float32)\n",
    "    pc_dev = ((c2 / c1) - 1)*100\n",
    "    avg_pc_deviation = np.mean(pc_dev)\n",
    "    for i, inst in enumerate(instances):\n",
    "        with open(os.path.join(parent_dir,\"results\",out+\"-\"+ heuristic+\"-pc-dev.txt\"), \"a\") as f:\n",
    "            f.write(str(inst)+' '+str(pc_dev[i])+'\\n')\n",
    "    print(f\"Average percent deviation from best known solutions ({heuristic}): {avg_pc_deviation:.2f} %\")\n",
    "\n",
    "instances = [\n",
    "    \"4.2\", \"4.3\", \"4.4\", \"4.5\", \"4.6\", \"4.7\", \"4.8\", \"4.9\",\n",
    "    \"5.1\", \"5.2\", \"5.3\", \"5.4\", \"5.5\", \"5.6\", \"5.7\", \"5.8\", \"5.9\",\n",
    "    \"6.1\", \"6.2\", \"6.3\", \"6.4\", \"6.5\",\n",
    "    \"A.1\", \"A.2\", \"A.3\", \"A.4\", \"A.5\",\n",
    "    \"B.1\", \"B.2\"\n",
    "    ]\n",
    "\n",
    "parent_dir = \"/home/jdnunes/Documents/Heuristics&MetaHeuristics/Assignment 2\"\n",
    "out = \"scp-ch4\"\n",
    "print_avg_pc_deviation(\"simulated-annealing\",parent_dir, out, instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
